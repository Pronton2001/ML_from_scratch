{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes classification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from math import pi, sqrt, exp\n",
    "from random import randrange, seed\n",
    "from csv import reader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dataset = [[3.393533211,2.331273381,0],\n",
    "\t[3.110073483,1.781539638,0],\n",
    "\t[1.343808831,3.368360954,0],\n",
    "\t[3.582294042,4.67917911,0],\n",
    "\t[2.280362439,2.866990263,0],\n",
    "\t[7.423436942,4.696522875,1],\n",
    "\t[5.745051997,3.533989803,1],\n",
    "\t[9.172168622,2.511101045,1],\n",
    "\t[7.792783481,3.424088941,1],\n",
    "\t[7.939820817,0.791637231,1]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def mean(X):\n",
    "    return sum(X)/ len(X)\n",
    "\n",
    "def stdev(X):\n",
    "\tmu = mean(X)\n",
    "\treturn  sum((x - mu)**2 for x in X) / (len(X) - 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "d = [[1, 3, 4],\n",
    "     [1, 2, 3],\n",
    "     [2, 3, 4],\n",
    "     [1, 2, 3]]\n",
    "d = [x[:-1]for x in d]\n",
    "print(*d)\n",
    "for column in zip(*d):\n",
    "    print(column)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 3] [1, 2] [2, 3] [1, 2]\n",
      "(1, 1, 2, 1)\n",
      "(3, 2, 3, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def summarize_data(dataset):\n",
    "\trm_last_col = [row[:-1] for row in dataset]\n",
    "\treturn [(mean(col), stdev(col), len(col)) for col in zip(*rm_last_col)]\n",
    "summarize_data(dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(5.178333386499999, 7.653989826170761, 10),\n",
       " (2.9984683241, 1.4848795625703213, 10)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def separated_by_class(dataset):\n",
    "\t\"\"\"\n",
    "\tSeparate the data set into classes\n",
    "\tAssume the final colulmn is ground truth of the class\n",
    "\t\"\"\"\n",
    "\tclasses = dict()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[-1] not in classes:\n",
    "\t\t\tclasses[row[-1]] = list()\n",
    "\t\tclasses[row[-1]].append(row)\n",
    "\treturn classes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for classes, rows in separated_by_class(dataset).items():\n",
    "\tprint(classes, rows)\n",
    "separated_by_class(dataset).items()\n",
    "\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separated_by_class(dataset)\n",
    "\tsumaries = dict()\n",
    "\tfor class_val, rows_class_val in separated.items():\n",
    "\t\tsumaries[class_val] = summarize_data(rows_class_val)\n",
    "\treturn sumaries\n",
    "print(summarize_by_class(dataset))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 [[3.393533211, 2.331273381, 0], [3.110073483, 1.781539638, 0], [1.343808831, 3.368360954, 0], [3.582294042, 4.67917911, 0], [2.280362439, 2.866990263, 0]]\n",
      "1 [[7.423436942, 4.696522875, 1], [5.745051997, 3.533989803, 1], [9.172168622, 2.511101045, 1], [7.792783481, 3.424088941, 1], [7.939820817, 0.791637231, 1]]\n",
      "{0: [(2.7420144012, 0.8585288681757653, 5), (3.0054686692, 1.2261788197598094, 5)], 1: [(7.6146523718, 1.5238227453753934, 5), (2.9914679790000003, 2.1146776839446155, 5)]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Probability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def gauss(x, mean, stdev):\n",
    "    if stdev == 0:\n",
    "        return 0\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return 1 / (sqrt(2 * pi) * stdev ) * exponent\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class Probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# summaries = summarize_by_class(dataset[:][:-2])\n",
    "# print([summaries[label][0][2] for label in summaries])\n",
    "# for class_val, sumary_class_val in summaries.items():\n",
    "#     print(class_val, sumary_class_val, '\\n')\n",
    "\n",
    "\n",
    "def MAP(dataset, input_row):\n",
    "    \"\"\"\n",
    "    Maximum a Posteriori (MAP) \n",
    "    h = argmax_h P(h|D) = argmax_h P(D|h) * P(h)\n",
    "    P(D|h) = P(D1|h) * P(D2|h) * ... * P(Dn|h)\n",
    "    \n",
    "    where, \n",
    "        D: real data\n",
    "        h: a hypothesis, in classification, a class\n",
    "        P(D|h) : Likelihood distribution of Data given a specific hypothesis, in classification,\n",
    "            a specific probability distribution and its parameters\n",
    "        P(h): Prior distribution\n",
    "        P(h|D): Most likely hypothesis given data, in classification, Most likely class given data\n",
    "    \"\"\"\n",
    "    total_rows = len(dataset)\n",
    "    summaries = summarize_by_class(dataset)\n",
    "    prob = dict()\n",
    "    max_prob, retClass = 0.0, None\n",
    "    for class_val, class_summaries in summaries.items():\n",
    "        print(class_val)\n",
    "        prob[class_val] = class_summaries[0][2] / float(total_rows)\n",
    "        # except the last column is ground truth\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            prob[class_val] *= gauss(input_row[i], mean, stdev)\n",
    "        if max_prob < prob[class_val]:\n",
    "            retClass = class_val\n",
    "            max_prob = prob[class_val]\n",
    "    return retClass, prob\n",
    "\n",
    "\n",
    "# MAP(dataset, [1, 2, 3])\n",
    "\n",
    "# Naive Bayes Algorithm\n",
    "\n",
    "def naive_bayes(train_set, test_set):\n",
    "    preidictions = list()\n",
    "    for row in test_set:\n",
    "        preidictions.append(MAP(train_set,row))\n",
    "    return preidictions\n",
    "\n",
    "# naive_bayes(dataset[:7], dataset[7:])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work with Real Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    file = open(filename, \"rt\")\n",
    "    lines = reader(file)\n",
    "    dataset = list(lines)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column])\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])  # concatenate lists of lists to a list\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            # test_set use to predict => no need to hold [class] data\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Make a prediction with Naive Bayes on Iris Dataset\n",
    "filename = 'data/sonar.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "# str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# fit model\n",
    "model = summarize_by_class(dataset)\n",
    "# define a new record\n",
    "row = [5.7,2.9,4.2,1.3]\n",
    "# predict the label\n",
    "label = predict(model, row)\n",
    "print('Data=%s, Predicted: %s' % (row, label))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1358/3379742757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# predict the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data=%s, Predicted: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mp_env': venv)"
  },
  "interpreter": {
   "hash": "28f96ec01d3331ab2b5d4a1bb32d02e762004829634cea2cd2eeaefdb8bb653c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
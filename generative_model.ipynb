{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generative Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes classification\n",
    "\n",
    "Asumption:\n",
    "\n",
    "- h ~ Bernoulli(phi)\n",
    "- D|h ~ Bernoulli(D)\n",
    "- Input data is conditional independent given y, i.e $p(D_i|D_j,y) = p(D_i|y)\\forall i,j$ \n",
    "\n",
    "With continous features problem, we can discretize a continous feature into ***k*** bounded ranges and then count them to find:\n",
    "\n",
    "$P(D_i=d_{ir}| h = 0)$ and $P(D_i=d_{ir}| h = 1) \\forall r \\in k=\\{<2,>=2 \\&<5,...\\}$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from math import pi, sqrt, exp, log, inf\n",
    "from random import randrange, seed\n",
    "from csv import reader\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "dataset = [[3.393533211, 2.331273381, 0],\n",
    "           [3.110073483, 1.781539638, 0],\n",
    "           [1.343808831, 3.368360954, 0],\n",
    "           [3.582294042, 4.67917911, 0],\n",
    "           [2.280362439, 2.866990263, 0],\n",
    "           [7.423436942, 4.696522875, 1],\n",
    "           [5.745051997, 3.533989803, 1],\n",
    "           [9.172168622, 2.511101045, 1],\n",
    "           [7.792783481, 3.424088941, 1],\n",
    "           [7.939820817, 0.791637231, 1]]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find mean, variance, length of each attributes (columns), except for the last column (ground truth value) - Wrong with Naive Bayes asumption"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def mean(X):\n",
    "    return sum(X)/ len(X)\n",
    "\n",
    "def stdev(X):\n",
    "\tmu = mean(X)\n",
    "\treturn  sum((x - mu)**2 for x in X) / (len(X) - 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "d = [[1, 3, 4],\n",
    "     [1, 2, 3],\n",
    "     [2, 3, 4],\n",
    "     [1, 2, 3]]\n",
    "print(*d)\n",
    "for column in zip(*d):\n",
    "    print(column)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 3, 4] [1, 2, 3] [2, 3, 4] [1, 2, 3]\n",
      "(1, 1, 2, 1)\n",
      "(3, 2, 3, 2)\n",
      "(4, 3, 4, 3)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def summarize_data(dataset):\n",
    "\t# except the last column is ground truth\n",
    "\trm_last_col = [row[:-1] for row in dataset]\n",
    "    \n",
    "    # zip(*A) = Transpose(A)\n",
    "\treturn [(mean(col), stdev(col), len(col)) for col in zip(*rm_last_col)]\n",
    "summarize_data(dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(5.178333386499999, 7.653989826170761, 10),\n",
       " (2.9984683241, 1.4848795625703213, 10)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def separated_by_class(dataset):\n",
    "\t\"\"\"\n",
    "\tSeparate the data set into classes\n",
    "\tAssume the final colulmn is ground truth of the class\n",
    "\t\"\"\"\n",
    "\tclasses = dict()\n",
    "\tfor row in dataset:\n",
    "\t\tif row[-1] not in classes:\n",
    "\t\t\tclasses[row[-1]] = list()\n",
    "\t\tclasses[row[-1]].append(row)\n",
    "\treturn classes\n",
    "separated_by_class(dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: [[3.393533211, 2.331273381, 0],\n",
       "  [3.110073483, 1.781539638, 0],\n",
       "  [1.343808831, 3.368360954, 0],\n",
       "  [3.582294042, 4.67917911, 0],\n",
       "  [2.280362439, 2.866990263, 0]],\n",
       " 1: [[7.423436942, 4.696522875, 1],\n",
       "  [5.745051997, 3.533989803, 1],\n",
       "  [9.172168622, 2.511101045, 1],\n",
       "  [7.792783481, 3.424088941, 1],\n",
       "  [7.939820817, 0.791637231, 1]]}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def summarize_by_class(dataset):\n",
    "\tseparated = separated_by_class(dataset)\n",
    "\tsumaries = dict()\n",
    "\tfor class_val, rows_class_val in separated.items(): # items() means (keys, values)\n",
    "\t\tsumaries[class_val] = summarize_data(rows_class_val)\n",
    "\treturn sumaries\n",
    "print(summarize_by_class(dataset))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: [(2.7420144012, 0.8585288681757653, 5), (3.0054686692, 1.2261788197598094, 5)], 1: [(7.6146523718, 1.5238227453753934, 5), (2.9914679790000003, 2.1146776839446155, 5)]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Probability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def gauss(x, mean, stdev):\n",
    "    assert stdev !=0\n",
    "    exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "    return 1 / (sqrt(2 * pi) * stdev ) * exponent\n",
    "\n",
    "def log_gausian(x, mean, stdev):\n",
    "    return - log(stdev) - 0.5 * (x-mean)**2/stdev**2 # removed constant: -0.5 * log(2*pi)\n",
    "\n",
    "def pro_smooth(Nh, N, p):\n",
    "    \"\"\"\n",
    "    Smooth technique\n",
    "    prob(X) = (Nh+mp) / (N + m)\n",
    "\n",
    "    where, \n",
    "        Nh: # occurences of the type A in class h\n",
    "        N: # of total types in class h\n",
    "        m: equivalent sample size (heuristic choice)\n",
    "        p: probability of the type A\n",
    "    \"\"\"\n",
    "    m = 3 \n",
    "    return (Nh+m*p)/(N + m)\n",
    "# log(gauss(0.014400, 0.022630, 0.000188)) will get zero. \n",
    "# Instead, we use log_gaussian that we computed by hand first.\n",
    "log_gausian(0.014400, 0.022630, 0.000188)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-949.6160989014708"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Class Probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def MAP(dataset, input_row):\n",
    "    \"\"\"\n",
    "    Maximum a Posteriori (MAP) \n",
    "    h = argmax_h P(h|D) = argmax_h P(D|h) * P(h)\n",
    "    P(D|h) = P(D1|h) * P(D2|h) * ... * P(Dn|h), assuming that these D1,D2,... is conditional independent\n",
    "    \n",
    "    where, \n",
    "        D: set of attributes/data/columns that contains {D1, D2,...}\n",
    "        h: a hypothesis, in classification, a class\n",
    "        P(D|h) : Likelihood distribution of data given a specific hypothesis, in classification,\n",
    "            likelihood distribution of data given a specific class\n",
    "        P(h): Prior distribution\n",
    "        P(h|D): Most likely hypothesis given data, in classification, most likely class given data\n",
    "    \"\"\"\n",
    "    total_rows = len(dataset)\n",
    "    summaries = summarize_by_class(dataset)\n",
    "    prob = dict()\n",
    "    max_prob, retClass = -inf, None\n",
    "    for class_val, class_summaries in summaries.items():\n",
    "        prob[class_val] = log(class_summaries[0][2] / float(total_rows))\n",
    "\n",
    "        for i in range(len(class_summaries)):\n",
    "            mean, stdev, _ = class_summaries[i]\n",
    "            prob[class_val] += log_gausian(input_row[i], mean, stdev)\n",
    "        if max_prob < prob[class_val]:\n",
    "            retClass = class_val\n",
    "            max_prob = prob[class_val]\n",
    "    return retClass, prob\n",
    "\n",
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train_set, test_set):\n",
    "    preidictions = list()\n",
    "    for row in test_set:\n",
    "        preidictions.append(MAP(train_set,row)[0])\n",
    "        # print(MAP(train_set,row))\n",
    "    return preidictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Version 0.3: Discretize contious data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def discretize(dataset):\n",
    "    \"\"\"\n",
    "    Discretize continous dataset into 3 small ranges {0,1,2} such that\n",
    "        - Range 1: R0 < diff/3\n",
    "        - Range 2: diff/3 =< R1 < diff *2/3\n",
    "        - Range 3: diff*2/3 <= R2\n",
    "    Args\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, np.ndarray):\n",
    "        dataset = np.array(dataset)\n",
    "    features = np.array([x[:-1] for x in dataset])\n",
    "\n",
    "    maxValue = np.max(features, axis = 0)\n",
    "    minValue = np.min(features, axis = 0)\n",
    "    diff= maxValue - minValue\n",
    "    \n",
    "    for row in dataset:\n",
    "        new_row = row [:-1] # remove last labeled column\n",
    "        idx0 = np.where(new_row - minValue < diff/3)[0]  \n",
    "        idx1 = np.where((new_row-minValue >= diff/3) & (new_row-minValue < diff*2/3))[0]\n",
    "        idx2 = np.where(new_row-minValue >= diff*2/3)[0]\n",
    "        \n",
    "        new_row[idx1] = [1]*len(idx1)\n",
    "        new_row[idx0] = [0]*len(idx0)\n",
    "        new_row[idx2] = [2]*len(idx2)\n",
    "    return dataset\n",
    "\n",
    "def count(col, labeled_col, c):\n",
    "    \"\"\"\n",
    "    Count the number of 3 ranges {0,1,2} in class [c]\n",
    "    Args: \n",
    "        col: (1xn) feature column\n",
    "        labeled_col: (1xn)\n",
    "        c: {0,1} class (There are 2 classes 0,1)\n",
    "    Return:\n",
    "        Number of 3 ranges {0,1,2} in class [c]\n",
    "    \"\"\"\n",
    "    count0 = count1 = count2 = 0\n",
    "    for value, class_val in zip(col, labeled_col):\n",
    "        if class_val == c:\n",
    "            if value == 0:\n",
    "                count0 += 1\n",
    "            elif value == 1:\n",
    "                count1 +=1\n",
    "            else:\n",
    "                count2 +=1\n",
    "    return count0, count1, count2\n",
    "\n",
    "def fit(X): # ???? -> fix return\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: (n x (d+1)) train_set\n",
    "    Return:\n",
    "        (3x2)x(dx1) arrays: Each array contains the probability of each range(R) in total 3 ranges{0,1,2} in 2 class{0,1}\n",
    "        such that:\n",
    "            R0 < diff/3\n",
    "            diff/3 =< R1 < diff *2/3\n",
    "            diff*2/3 <= R2\n",
    "    \"\"\"\n",
    "    total = len(X)\n",
    "    last_col = [x[-1] for x in X]\n",
    "    total_1 = sum(last_col) # sum all class=1 to count\n",
    "    total_0 = total - total_1\n",
    "    \n",
    "    rm_last_col = [row[:-1] for row in X]\n",
    "    # Discretize train_set\n",
    "    rm_last_col = discretize(rm_last_col)\n",
    "    \n",
    "    # Initialize \n",
    "    R0_0 = R0_1 = R0_2 = np.array([])\n",
    "    R1_0 = R1_1 = R1_2 = np.array([])\n",
    "\n",
    "    for col in zip(*rm_last_col):\n",
    "        c0, c1, c2 = np.array(count(col, last_col, 0)) / total_0\n",
    "        R0_0 = np.append(R0_0,c0)\n",
    "        R0_1 = np.append(R0_1,c1)\n",
    "        R0_2 = np.append(R0_2,c2)\n",
    "        \n",
    "        t0, t1, t2 = np.array(count(col, last_col, 1)) / total_1\n",
    "        R1_0 = np.append(R1_0,t0)\n",
    "        R1_1 = np.append(R1_1,t1)\n",
    "        R1_2 = np.append(R1_2,t2)\n",
    "    return (R0_0, R0_1, R0_2), (R1_0, R1_1, R1_2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def compute_prob(test_set, G, fi, c):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test_set: (n x d) [discretized and removed the last column]\n",
    "        G: (1 x 3) count probability of 3 ranges {0,1,2} in class [c]{0,1}\n",
    "        c: (scalar) class [c]\n",
    "    Return:\n",
    "        Product of Probability: (n x 1). Formula: $P(D = {0,1,2}|c={0,1})P(c={0,1})$\n",
    "    \"\"\"\n",
    "    prob = []\n",
    "    phi = fi**c * (1-fi)**(1-c)\n",
    "    for row in test_set:\n",
    "        prob_each_row = phi\n",
    "        for i, val in enumerate(row):\n",
    "            if val == 0:\n",
    "                prob_each_row*= G[0][i]\n",
    "            if val == 1:\n",
    "                prob_each_row*= G[1][i]\n",
    "            if val == 2:\n",
    "                prob_each_row*= G[2][i]\n",
    "        prob.append(prob_each_row)\n",
    "    return prob\n",
    "\n",
    "def predict(X, G, fi):\n",
    "    return np.argmax([compute_prob(X, G[c], fi, c) for c in [0,1]], axis = 0)\n",
    "\n",
    "def naive_bayes_fixed(train_set, test_set):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_set: (n x (d+1)), with [d] features and 1 label column\n",
    "        test_set: (n x (d+1)), with [d] features and 1 label column\n",
    "    Return:\n",
    "        A list of predictions: (1 x n)\n",
    "    \"\"\" \n",
    "    y = np.array([x[-1] for x in train_set])\n",
    "    test_set = np.array([x[:-1] for x in test_set])\n",
    "    test_set = discretize(test_set)\n",
    "    fi = y.mean()\n",
    "    G = fit(train_set)\n",
    "    return predict(test_set, G, fi)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Work with Real Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    file = open(filename, \"rt\")\n",
    "    lines = reader(file)\n",
    "    dataset = list(lines)\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column])\n",
    "        \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for i in range(n_folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])  # concatenate lists of lists to a list\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            # test_set use to predict => no need to hold [class] data\n",
    "            row_copy[-1] = None\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "seed(1)\n",
    "# Make a prediction with Naive Bayes on Iris Dataset\n",
    "\n",
    "# filename = 'data/sonar.csv'\n",
    "filename = 'data/BankNote_Authentication.csv'\n",
    "dataset = load_csv(filename)\n",
    "\n",
    "# remove the string [attributes]\n",
    "dataset.pop(0)\n",
    "\n",
    "for i in range(len(dataset[0])-1): # except for the last column\n",
    "\tstr_column_to_float(dataset, i)\n",
    "    \n",
    "# Check if the last row is not INT{0,1}\n",
    "if not isinstance(dataset[0][-1], int):\n",
    "\tprint(\"The last row convert from str to int\")\n",
    "\tstr_column_to_int(dataset, len(dataset[0])-1)\n",
    "    \n",
    "# fit model\n",
    "model = summarize_by_class(dataset)\n",
    "\n",
    "# predict the label\n",
    "n_folds = 5\n",
    "\n",
    "scores = evaluate_algorithm(dataset, naive_bayes_fixed, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The last row convert from str to int\n",
      "Scores: [88.32116788321169, 89.78102189781022, 88.6861313868613, 89.78102189781022, 91.24087591240875]\n",
      "Mean Accuracy: 89.562%\n"
     ]
    }
   ],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Verion 0.1:\n",
    "- This model cannot work with huge attributes, i.e `data/sonar.csv` due to the multiplication increase when # of attributes increase. The probability of each class rapidly converges to zero and we cannot use it to the the Maximum a Posteriori (MAP).\n",
    "\n",
    "- The probability of these classes:\n",
    "```python\n",
    "    2.9139230184726384e-13\n",
    "    1.0403767988304062e-181\n",
    "    0.0\n",
    "    0.0\n",
    "    ...\n",
    "```\n",
    "\n",
    "Version 0.2:\n",
    "- Accuracy:\n",
    "    - 50% data\\sonar.csv\n",
    "- Version 0.1 's Problem: numercal unstability\n",
    "    - Fix: use log to avoid the numerical problem\n",
    "\n",
    "```python\n",
    "    # prob[class_val] = (class_summaries[0][2] / float(total_rows))\n",
    "    prob[class_val] = log(class_summaries[0][2] / float(total_rows)\n",
    "    ... \n",
    "    # prob[class_val] *= (gauss(input_row[i], mean, stdev))\n",
    "    prob[class_val] += log_gausian(input_row[i], mean, stdev)\n",
    "```\n",
    "* However, the average accuracy of this model on the `data\\sonar.csv` dataset is just over 50% that is the same as the guessing.\n",
    "\n",
    "Version 0.3: 9/8/2021\n",
    "\n",
    "* Change to right assumption\n",
    "* Dicretize continous data into 3 small splits, namely range 1,2,3\n",
    "* This version follows Naive Bayes assumption rather than the previous version that combine Naive Bayes and Gaussian Discriminant Analysis assumptions.\n",
    "* Accuracy: \n",
    "    - 71.220% data/sonar.csv\n",
    "    - 89.562% data/BankNote_Authentication.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gaussian Discriminnat Analaysis (Linear)\n",
    "\n",
    "Assumption:\n",
    "\n",
    "- y ~ Bernoulli($\\phi$) - 1 dimension\n",
    "- x|y = 0 ~ Gaussian($\\mu_0, \\Sigma$) - d dimension\n",
    "- x|y = 1 ~ Gaussian($\\mu_1, \\Sigma$) - d dimension"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "X = np.array([[1,2,3,1],[1,2,2,1],[0,1,4,0]], np.float32)\n",
    "y = np.array([x[-1] for x in X])\n",
    "# print(X.shape, y.shape)\n",
    "mean = [X[y==k].mean(axis = 0) for k in [0,1]]\n",
    "mean"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([0., 1., 4., 0.], dtype=float32),\n",
       " array([1. , 2. , 2.5, 1. ], dtype=float32)]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "X_u = X.copy()\n",
    "\n",
    "for k in [0,1]: X_u[y==k] -= mean[k]\n",
    "stddev = X_u.T.dot(X_u) / len(y)\n",
    "invStddev = np.linalg.pinv(stddev)\n",
    "invStddev\n",
    "len(mean)\n",
    "# mean[1]\n",
    "# X_u[y==1] -= mean[1]\n",
    "X = [x[:-1] for x in X]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def fit(X, y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: A list with (n x d); n datasize & [d] value (features)\n",
    "        y: A list with (n x 1); n datasize & 1 value (label)\n",
    "    Return:\n",
    "        mean: (2 x d) (2 because we have 2 classes)\n",
    "        covariance: (d x d) = 1/n * SUM\n",
    "        fi: scalar (because we have 2 classes, fi = P(y = 1) = 1 - P(y = 0))\n",
    "    \"\"\"\n",
    "    mean = np.array([X[y == k].mean(axis=0) for k in [0, 1]])\n",
    "    X_u = X.copy()  # X_u: (n x d)\n",
    "    for k in [0, 1]:\n",
    "        X_u[y == k] -= mean[k]\n",
    "        \n",
    "    covariance = X_u.T.dot(X_u) / len(y) # X_u.T(d x n) * X_u(n x d) / n    -> (dxd)\n",
    "    fi = y.mean()\n",
    "    return mean, covariance, fi\n",
    "\n",
    "\n",
    "def predict(X, u, invE, fi):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            X: (n x d)\n",
    "            u: (2 x d)\n",
    "            invE: (d x d)\n",
    "            fi: scalar\n",
    "        Return:\n",
    "            Array of [n] predictions\n",
    "    \"\"\"\n",
    "    return np.argmax([compute_prob(X, u[c], invE, fi, c) for c in [0, 1]], axis=0)\n",
    "\n",
    "\n",
    "def compute_prob(X, u, invE, fi, c):\n",
    "    \"\"\"\n",
    "    X: dataset, the last column is the ground truth\n",
    "    c: class\n",
    "\n",
    "    Return: P(X,c) = P(X|c)P(c)\n",
    "        where,\n",
    "        P(X|c) ~ Gaussian(u, E)\n",
    "        P(c) ~ Berounlli(phi)\n",
    "    \"\"\"\n",
    "    phi = fi**c * (1-fi)**(1-c)\n",
    "    # return np.exp(-1.0 * np.sum((X - u).dot(invE)*(X - u), axis=1)) * phi    # sum up the row/features\n",
    "    return -1.0/2.0* np.sum((X-u).dot(invE)*(X-u),axis=1) + log(phi) # sum up the row/features\n",
    "\n",
    "\n",
    "def Binary_GDA(train_set, test_set):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_set: (n x (d+1)), with [d] features and 1 label column\n",
    "        test_set: (n x (d+1)), with [d] features and 1 label column\n",
    "    Return:\n",
    "        A list of predictions: (1 x n)\n",
    "    \"\"\" \n",
    "    y = np.array([x[-1] for x in train_set])\n",
    "    X = np.array([x[:-1] for x in train_set])\n",
    "    test_set = np.array([x[:-1] for x in test_set])\n",
    "\n",
    "    u, E, fi = fit(X, y)\n",
    "    invE = np.linalg.pinv(E)\n",
    "    return predict(test_set, u, invE, fi)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "X = np.array([[1, 2, 3, 1, 1],\n",
    "              [1, 2, 2, 3, 1],\n",
    "              [0, 1, 4, 1, 0],\n",
    "              [1, 1, 1, 3, 1],\n",
    "              [1, 2, 3, 4, 0]], np.float32)\n",
    "\n",
    "y = np.array([x[-1] for x in X])\n",
    "X = np.array([x[:-1] for x in X])\n",
    "\n",
    "u, E, fi = fit(X, y)\n",
    "invE = np.linalg.pinv(E)\n",
    "phi0 = 1 - fi\n",
    "phi1 = fi\n",
    "\n",
    "# why element-wise product -> valid to compute\n",
    "# why sum?\n",
    "L0 = np.exp(-1.0 * np.sum((X - u[0]).dot(invE)*(X-u[0]),axis=1) ) * phi0\n",
    "\n",
    "L1 = np.exp(-1.0 * np.sum((X - u[1]).dot(invE)*(X-u[1]), axis=1) ) * phi1\n",
    "\n",
    "print(np.argmax([L0, L1], axis=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 0 1 0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# seed(1)\n",
    "# Make a prediction with Naive Bayes on Iris Dataset\n",
    "filename = 'data/sonar.csv'\n",
    "filename = 'data/BankNote_Authentication.csv'\n",
    "\n",
    "dataset = load_csv(filename)\n",
    "\n",
    "# remove the string [attributes]\n",
    "dataset.pop(0)\n",
    "\n",
    "for i in range(len(dataset[0])-1): # except for the last column\n",
    "\tstr_column_to_float(dataset, i)\n",
    "\n",
    "# Check if the last row is not INT{0,1}\n",
    "if not isinstance(dataset[0][-1], int):\n",
    "\tprint(\"The last row convert from str to int\")\n",
    "\tstr_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# predict the label\n",
    "n_folds = 5\n",
    "\n",
    "scores = evaluate_algorithm(dataset, Binary_GDA, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The last row convert from str to int\n",
      "Scores: [98.17518248175182, 97.08029197080292, 97.08029197080292, 97.08029197080292, 98.90510948905109]\n",
      "Mean Accuracy: 97.664%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Version 0.1:\n",
    "* Add Binary_GDA(linear):\n",
    "    - 72.683% data/sonar.csv\n",
    "    - 97.664% data/BankNote_Authentication.csv\n",
    "This model has the same accuracy with decision tree model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Refs\n",
    "1. https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
    "2. https://github.com/bamtak/machine-learning-implemetation-python/blob/master/Gaussian%20Discriminant%20Analyses.ipynb"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28f96ec01d3331ab2b5d4a1bb32d02e762004829634cea2cd2eeaefdb8bb653c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mp_env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}